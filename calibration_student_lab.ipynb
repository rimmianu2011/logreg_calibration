{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CazTw2WjgSId"
      },
      "source": [
        "# Logistic Regression & Calibration — Student Lab\n",
        "\n",
        "We focus on *probabilities*, not just accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dbGvX0LcgSIe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def check(name: str, cond: bool):\n",
        "    if not cond:\n",
        "        raise AssertionError(f'Failed: {name}')\n",
        "    print(f'OK: {name}')\n",
        "\n",
        "rng = np.random.default_rng(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZBNu2rWgSIe"
      },
      "source": [
        "## Section 0 — Synthetic imbalanced data\n",
        "We simulate logits and labels with imbalance and miscalibration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4J5ijsz9gSIf",
        "outputId": "96211ae0-33ba-4911-d7b7-02528a1749b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "base_rate 0.0712\n",
            "OK: in_0_1\n"
          ]
        }
      ],
      "source": [
        "def make_probs(n=5000, base_rate=0.05, logit_scale=1.0, miscalibration=1.0):\n",
        "    # Generate true probabilities via latent logits\n",
        "    z = logit_scale * rng.standard_normal(n)\n",
        "    # shift to get desired base rate approximately\n",
        "    z = z + np.log(base_rate/(1-base_rate))\n",
        "    p_true = 1/(1+np.exp(-z))\n",
        "    y = (rng.random(n) < p_true).astype(int)\n",
        "    # observed model probs are miscalibrated by scaling logits\n",
        "    z_model = miscalibration * z\n",
        "    p_model = 1/(1+np.exp(-z_model))\n",
        "    return y, p_model, p_true\n",
        "\n",
        "y, p_model, p_true = make_probs(miscalibration=2.0)\n",
        "print('base_rate', y.mean())\n",
        "check('in_0_1', np.all((p_model>=0) & (p_model<=1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kEhnLfGgSIf"
      },
      "source": [
        "## Section 1 — Metrics\n",
        "\n",
        "### Task 1.1: Confusion matrix metrics at a threshold\n",
        "Implement precision, recall, F1 at threshold t.\n",
        "\n",
        "# HINT:\n",
        "- y_hat = (p>=t)\n",
        "- TP/FP/FN\n",
        "\n",
        "**Checkpoint:** Why is accuracy misleading under imbalance?\n",
        "\n",
        "**Answer:** Accuracy counts all correct predictions equally. Under class imbalance (for instance, 99% negatives, 1% positives), a trivial model that predicts everything as negative achieves 99% accuracy, yet it completely fails at the task of finding positives (TP = 0, recall = 0). Accuracy is dominated by the majority class (TN), so it hides poor performance on the minority class that usually matters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVXEWi3tgSIf",
        "outputId": "182bf353-13ce-4424-8b84-581e340a015b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'tp': 2, 'fp': 2, 'fn': 354, 'recall': 0.005617977528089872, 'precision': 0.499999999999875, 'F1 score': 0.011111111111089075}\n"
          ]
        }
      ],
      "source": [
        "def metrics_at_threshold(y, p, t):\n",
        "    # TODO\n",
        "    y = y.astype(int)\n",
        "    y_hat = (p >= t).astype(int)\n",
        "    tp = int(np.sum((y_hat == 1) & (y == 1)))\n",
        "    fp = int(np.sum((y_hat == 1) & (y == 0)))\n",
        "    fn = int(np.sum((y_hat == 0) & (y == 1)))\n",
        "    # tn = int(np.sum((y_hat == 0) && (y == 0)))\n",
        "\n",
        "    precision = tp / (tp + fp + 1e-12)\n",
        "    recall = tp / (tp + fn + 1e-12)\n",
        "    F1_score = 2 * precision * recall / (precision + recall + 1e-12)\n",
        "    return {'tp':tp, 'fp':fp, 'fn':fn, 'recall':recall, 'precision':precision, 'F1 score':F1_score }\n",
        "\n",
        "m = metrics_at_threshold(y, p_model, t=0.5)\n",
        "print(m)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JwpVqzBgSIf"
      },
      "source": [
        "### Task 1.2: PR curve area (approx)\n",
        "Compute a simple PR-AUC approximation by sorting thresholds.\n",
        "\n",
        "# HINT:\n",
        "- sort by p desc\n",
        "- compute precision/recall at each cut\n",
        "\n",
        "**Interview Angle:** when is PR-AUC preferable to ROC-AUC?\n",
        "\n",
        "**Answer:** PR-AUC is preferable to ROC-AUC when the positive class is rare and we care about performance on positives (finding true cases and avoiding false alarms).\n",
        "- Severe class imbalance\n",
        "- False positives are costly\n",
        "- We care about “positive usefulness”\n",
        "- PR-AUC reflects real trade-offs between precision and recall WHERE AS ROC-AUC can look good even when no usable threshold exists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJN_1YAWgSIf",
        "outputId": "ff0e35fe-c451-4581-ed56-e07e0603aeaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pr_auc 0.20170158178004188\n",
            "OK: finite\n"
          ]
        }
      ],
      "source": [
        "def pr_curve(y, p):\n",
        "    # TODO: return arrays (recall, precision)\n",
        "    order = np.argsort(-p)\n",
        "    y_sorted = y[order]\n",
        "    tp = np.cumsum(y_sorted == 1)\n",
        "    fp = np.cumsum(y_sorted == 0)\n",
        "    # print(tp, fp)\n",
        "    precision = tp / (tp + fp + 1e-12)\n",
        "    recall = tp / (tp[-1] + 1e-12)\n",
        "    return recall, precision\n",
        "\n",
        "def auc_trapz(x, y):\n",
        "    # TODO\n",
        "    return float(np.trapezoid(y, x))\n",
        "\n",
        "rec, prec = pr_curve(y, p_model)\n",
        "pr_auc = auc_trapz(rec, prec)\n",
        "print('pr_auc', pr_auc)\n",
        "check('finite', np.isfinite(pr_auc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhuc4aYMgSIf"
      },
      "source": [
        "## Section 2 — Calibration\n",
        "\n",
        "### Task 2.1: Reliability curve + ECE\n",
        "\n",
        "Bin probabilities and compute:\n",
        "- avg predicted prob per bin\n",
        "- empirical accuracy per bin\n",
        "- ECE = sum (bin_weight * |acc - conf|)\n",
        "\n",
        "# HINT:\n",
        "- np.digitize\n",
        "\n",
        "**FAANG gotcha:** model can have good ranking but bad calibration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nl7yyAFLgSIg",
        "outputId": "12929de7-3fbd-4487-f553-52680f8d3a8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ECE 0.056267450617459955\n",
            "OK: ece_finite\n"
          ]
        }
      ],
      "source": [
        "def reliability_bins(y, p, n_bins=10):\n",
        "    # TODO: return (bin_acc, bin_conf, bin_frac)\n",
        "    y = y.astype(int)\n",
        "    edges = np.linspace(0, 1, n_bins + 1)\n",
        "    b = np.digitize(p, edges[1:-1], right=False)\n",
        "\n",
        "    bin_accuracy = np.zeros(n_bins)\n",
        "    bin_configuration = np.zeros(n_bins)\n",
        "    bin_fraction = np.zeros(n_bins)\n",
        "\n",
        "    for i in range(n_bins):\n",
        "      mask = (b == i)\n",
        "      if mask.any():\n",
        "        bin_accuracy[i] = y[mask].mean()\n",
        "        bin_configuration[i] = p[mask].mean()\n",
        "        bin_fraction[i] = mask.mean()\n",
        "\n",
        "    return bin_accuracy, bin_configuration, bin_fraction\n",
        "\n",
        "\n",
        "def ece(bin_acc, bin_conf, bin_frac):\n",
        "    # TODO\n",
        "    return float(np.sum(bin_frac * np.abs(bin_acc - bin_conf)))\n",
        "\n",
        "bin_acc, bin_conf, bin_frac = reliability_bins(y, p_model, n_bins=10)\n",
        "ECE = ece(bin_acc, bin_conf, bin_frac)\n",
        "print('ECE', ECE)\n",
        "check('ece_finite', np.isfinite(ECE))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9sJLbOlgSIg"
      },
      "source": [
        "### Task 2.2: Temperature scaling\n",
        "\n",
        "We assume p_model came from logits z_model. Approximate logits via logit(p).\n",
        "Then find temperature T that minimizes NLL on validation split: sigmoid(z/T).\n",
        "\n",
        "# HINT:\n",
        "- logit(p)=log(p/(1-p))\n",
        "- grid search T over [0.5..5]\n",
        "\n",
        "**Checkpoint:** Why does scaling logits preserve ranking?\n",
        "\n",
        "**Answer:** Because temperature scaling only changes confidence, not order.\n",
        "- The model already decides which examples are more likely than others using logits (scores).\n",
        "- Dividing all logits by the same positive number T keeps their relative order the same.\n",
        "- A higher logit before scaling is still higher after scaling.\n",
        "- The sigmoid function also preserves this order when converting logits to probabilities.\n",
        "\n",
        "So, predictions stay ranked the same; only the probabilities become softer or sharper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuOkwnFkgSIg",
        "outputId": "f3bac83a-3f9a-4f4e-e971-f9f39a2efb5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best_T 1.969387755102041\n",
            "ECE_before 0.05789031077500049 ECE_after 0.006327853058047911\n"
          ]
        }
      ],
      "source": [
        "def logit(p, eps=1e-12):\n",
        "    p = np.clip(p, eps, 1-eps)\n",
        "    return np.log(p/(1-p))\n",
        "\n",
        "def nll(y, p, eps=1e-12):\n",
        "    p = np.clip(p, eps, 1-eps)\n",
        "    return float(-np.mean(y*np.log(p) + (1-y)*np.log(1-p)))\n",
        "\n",
        "# TODO: split into val and fit T\n",
        "idx = rng.permutation(len(y))\n",
        "val = idx[: len(y)//2]\n",
        "test = idx[len(y)//2:]\n",
        "\n",
        "z = logit(p_model)\n",
        "\n",
        "Ts = np.linspace(0.5, 5.0, 50)\n",
        "best_T = None\n",
        "best_loss = float('inf')\n",
        "for T in Ts:\n",
        "  pT = 1 / (1 + np.exp(-(z[val] / T)))\n",
        "  L = nll(y[val], pT)\n",
        "  if L < best_loss:\n",
        "    best_loss = L\n",
        "    best_T = T\n",
        "# p_cal = 1 / (1 + np.exp(-z[test] / best_T))\n",
        "\n",
        "print('best_T', best_T)\n",
        "# apply temperature on test\n",
        "p_cal = 1 / (1 + np.exp(-(z[test] / best_T)))\n",
        "ECE_before = ece(*reliability_bins(y[test], p_model[test], 10))\n",
        "ECE_after = ece(*reliability_bins(y[test], p_cal, 10))\n",
        "print('ECE_before', ECE_before, 'ECE_after', ECE_after)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKUfu0i4gSIg"
      },
      "source": [
        "## Section 3 — Thresholding with costs\n",
        "\n",
        "### Task 3.1: Pick threshold minimizing cost\n",
        "Cost = c_fp*FP + c_fn*FN\n",
        "\n",
        "# TODO: sweep thresholds and pick best.\n",
        "\n",
        "**Interview Angle:** map model probabilities to business decisions.\n",
        "\n",
        "**Explanation:** Model probabilities are not decisions.\n",
        "A threshold basically turns them into actions. The “best” threshold depends on what the business hates more:\n",
        "- If missing a positive is expensive then choose a lower threshold (catch more, accept more false alarms)\n",
        "- If false alarms are expensive then choose a higher threshold (fewer alerts, miss more)\n",
        "\n",
        "Overall, optimize for cost and not accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf1_fYSLgSIg",
        "outputId": "bffe829f-3b58-41ba-a28d-2c8c718013da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t* 0.006 cost 2626.0\n"
          ]
        }
      ],
      "source": [
        "def best_threshold_cost(y, p, c_fp=1.0, c_fn=10.0):\n",
        "    # TODO\n",
        "    ts = np.linspace(0, 1, 501)\n",
        "    best_T = 0.5\n",
        "    best_cost = float('inf')\n",
        "\n",
        "    for t in ts:\n",
        "      y_hat = (p >= t).astype(int)\n",
        "      fp = np.sum((y_hat == 1) & (y == 0))\n",
        "      fn = np.sum((y_hat == 0) & (y == 1))\n",
        "      cost = c_fp * fp + c_fn * fn\n",
        "\n",
        "      if cost < best_cost:\n",
        "        best_cost = cost\n",
        "        best_T = float(t)\n",
        "\n",
        "    return best_T, float(best_cost)\n",
        "\n",
        "t_star, cost_star = best_threshold_cost(y, p_model, c_fp=1.0, c_fn=10.0)\n",
        "print('t*', t_star, 'cost', cost_star)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBbfiwSMgSIg"
      },
      "source": [
        "---\n",
        "## Submission Checklist\n",
        "- All TODOs completed\n",
        "- ECE computed\n",
        "- Temperature scaling applied\n",
        "- Threshold recommendation written\n"
      ]
    }
  ]
}