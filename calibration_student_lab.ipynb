{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CazTw2WjgSId"
      },
      "source": [
        "# Logistic Regression & Calibration — Student Lab\n",
        "\n",
        "We focus on *probabilities*, not just accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dbGvX0LcgSIe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def check(name: str, cond: bool):\n",
        "    if not cond:\n",
        "        raise AssertionError(f'Failed: {name}')\n",
        "    print(f'OK: {name}')\n",
        "\n",
        "rng = np.random.default_rng(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZBNu2rWgSIe"
      },
      "source": [
        "## Section 0 — Synthetic imbalanced data\n",
        "We simulate logits and labels with imbalance and miscalibration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4J5ijsz9gSIf",
        "outputId": "66ac109a-d77a-432e-e31f-3a8cedf15a7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "base_rate 0.0712\n",
            "OK: in_0_1\n"
          ]
        }
      ],
      "source": [
        "def make_probs(n=5000, base_rate=0.05, logit_scale=1.0, miscalibration=1.0):\n",
        "    # Generate true probabilities via latent logits\n",
        "    z = logit_scale * rng.standard_normal(n)\n",
        "    # shift to get desired base rate approximately\n",
        "    z = z + np.log(base_rate/(1-base_rate))\n",
        "    p_true = 1/(1+np.exp(-z))\n",
        "    y = (rng.random(n) < p_true).astype(int)\n",
        "    # observed model probs are miscalibrated by scaling logits\n",
        "    z_model = miscalibration * z\n",
        "    p_model = 1/(1+np.exp(-z_model))\n",
        "    return y, p_model, p_true\n",
        "\n",
        "y, p_model, p_true = make_probs(miscalibration=2.0)\n",
        "print('base_rate', y.mean())\n",
        "check('in_0_1', np.all((p_model>=0) & (p_model<=1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kEhnLfGgSIf"
      },
      "source": [
        "## Section 1 — Metrics\n",
        "\n",
        "### Task 1.1: Confusion matrix metrics at a threshold\n",
        "Implement precision, recall, F1 at threshold t.\n",
        "\n",
        "# HINT:\n",
        "- y_hat = (p>=t)\n",
        "- TP/FP/FN\n",
        "\n",
        "**Checkpoint:** Why is accuracy misleading under imbalance?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OVXEWi3tgSIf",
        "outputId": "bd9edde6-1637-4e66-87a4-8fce46bc0e43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'tp': 2, 'fp': 2, 'fn': 354, 'recall': 0.005617977528089872, 'precision': 0.499999999999875, 'F1 score': 0.011111111111089075}\n"
          ]
        }
      ],
      "source": [
        "def metrics_at_threshold(y, p, t):\n",
        "    # TODO\n",
        "    y = y.astype(int)\n",
        "    y_hat = (p >= t).astype(int)\n",
        "    tp = int(np.sum((y_hat == 1) & (y == 1)))\n",
        "    fp = int(np.sum((y_hat == 1) & (y == 0)))\n",
        "    fn = int(np.sum((y_hat == 0) & (y == 1)))\n",
        "    # tn = int(np.sum((y_hat == 0) && (y == 0)))\n",
        "\n",
        "    precision = tp / (tp + fp + 1e-12)\n",
        "    recall = tp / (tp + fn + 1e-12)\n",
        "    F1_score = 2 * precision * recall / (precision + recall + 1e-12)\n",
        "    return {'tp':tp, 'fp':fp, 'fn':fn, 'recall':recall, 'precision':precision, 'F1 score':F1_score }\n",
        "\n",
        "m = metrics_at_threshold(y, p_model, t=0.5)\n",
        "print(m)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JwpVqzBgSIf"
      },
      "source": [
        "### Task 1.2: PR curve area (approx)\n",
        "Compute a simple PR-AUC approximation by sorting thresholds.\n",
        "\n",
        "# HINT:\n",
        "- sort by p desc\n",
        "- compute precision/recall at each cut\n",
        "\n",
        "**Interview Angle:** when is PR-AUC preferable to ROC-AUC?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJN_1YAWgSIf"
      },
      "outputs": [],
      "source": [
        "def pr_curve(y, p):\n",
        "    # TODO: return arrays (recall, precision)\n",
        "    ...\n",
        "\n",
        "def auc_trapz(x, y):\n",
        "    # TODO\n",
        "    ...\n",
        "\n",
        "rec, prec = pr_curve(y, p_model)\n",
        "pr_auc = auc_trapz(rec, prec)\n",
        "print('pr_auc', pr_auc)\n",
        "check('finite', np.isfinite(pr_auc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhuc4aYMgSIf"
      },
      "source": [
        "## Section 2 — Calibration\n",
        "\n",
        "### Task 2.1: Reliability curve + ECE\n",
        "\n",
        "Bin probabilities and compute:\n",
        "- avg predicted prob per bin\n",
        "- empirical accuracy per bin\n",
        "- ECE = sum (bin_weight * |acc - conf|)\n",
        "\n",
        "# HINT:\n",
        "- np.digitize\n",
        "\n",
        "**FAANG gotcha:** model can have good ranking but bad calibration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nl7yyAFLgSIg"
      },
      "outputs": [],
      "source": [
        "def reliability_bins(y, p, n_bins=10):\n",
        "    # TODO: return (bin_acc, bin_conf, bin_frac)\n",
        "    ...\n",
        "\n",
        "def ece(bin_acc, bin_conf, bin_frac):\n",
        "    # TODO\n",
        "    ...\n",
        "\n",
        "bin_acc, bin_conf, bin_frac = reliability_bins(y, p_model, n_bins=10)\n",
        "ECE = ece(bin_acc, bin_conf, bin_frac)\n",
        "print('ECE', ECE)\n",
        "check('ece_finite', np.isfinite(ECE))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9sJLbOlgSIg"
      },
      "source": [
        "### Task 2.2: Temperature scaling\n",
        "\n",
        "We assume p_model came from logits z_model. Approximate logits via logit(p).\n",
        "Then find temperature T that minimizes NLL on validation split: sigmoid(z/T).\n",
        "\n",
        "# HINT:\n",
        "- logit(p)=log(p/(1-p))\n",
        "- grid search T over [0.5..5]\n",
        "\n",
        "**Checkpoint:** Why does scaling logits preserve ranking?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuOkwnFkgSIg"
      },
      "outputs": [],
      "source": [
        "def logit(p, eps=1e-12):\n",
        "    p = np.clip(p, eps, 1-eps)\n",
        "    return np.log(p/(1-p))\n",
        "\n",
        "def nll(y, p, eps=1e-12):\n",
        "    p = np.clip(p, eps, 1-eps)\n",
        "    return float(-np.mean(y*np.log(p) + (1-y)*np.log(1-p)))\n",
        "\n",
        "# TODO: split into val and fit T\n",
        "idx = rng.permutation(len(y))\n",
        "val = idx[: len(y)//2]\n",
        "test = idx[len(y)//2:]\n",
        "\n",
        "z = logit(p_model)\n",
        "\n",
        "best_T = ...\n",
        "\n",
        "print('best_T', best_T)\n",
        "# apply temperature on test\n",
        "p_cal = 1/(1+np.exp(-(z[test]/best_T)))\n",
        "ECE_before = ece(*reliability_bins(y[test], p_model[test], 10))\n",
        "ECE_after = ece(*reliability_bins(y[test], p_cal, 10))\n",
        "print('ECE_before', ECE_before, 'ECE_after', ECE_after)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKUfu0i4gSIg"
      },
      "source": [
        "## Section 3 — Thresholding with costs\n",
        "\n",
        "### Task 3.1: Pick threshold minimizing cost\n",
        "Cost = c_fp*FP + c_fn*FN\n",
        "\n",
        "# TODO: sweep thresholds and pick best.\n",
        "\n",
        "**Interview Angle:** map model probabilities to business decisions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bf1_fYSLgSIg"
      },
      "outputs": [],
      "source": [
        "def best_threshold_cost(y, p, c_fp=1.0, c_fn=10.0):\n",
        "    # TODO\n",
        "    ...\n",
        "\n",
        "t_star, cost_star = best_threshold_cost(y, p_model, c_fp=1.0, c_fn=10.0)\n",
        "print('t*', t_star, 'cost', cost_star)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBbfiwSMgSIg"
      },
      "source": [
        "---\n",
        "## Submission Checklist\n",
        "- All TODOs completed\n",
        "- ECE computed\n",
        "- Temperature scaling applied\n",
        "- Threshold recommendation written\n"
      ]
    }
  ]
}