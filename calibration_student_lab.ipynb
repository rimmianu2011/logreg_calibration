{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CazTw2WjgSId"
      },
      "source": [
        "# Logistic Regression & Calibration — Student Lab\n",
        "\n",
        "We focus on *probabilities*, not just accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dbGvX0LcgSIe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def check(name: str, cond: bool):\n",
        "    if not cond:\n",
        "        raise AssertionError(f'Failed: {name}')\n",
        "    print(f'OK: {name}')\n",
        "\n",
        "rng = np.random.default_rng(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZBNu2rWgSIe"
      },
      "source": [
        "## Section 0 — Synthetic imbalanced data\n",
        "We simulate logits and labels with imbalance and miscalibration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4J5ijsz9gSIf",
        "outputId": "101dbe2b-2867-49d2-87b8-3d496248a4ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "base_rate 0.0712\n",
            "OK: in_0_1\n"
          ]
        }
      ],
      "source": [
        "def make_probs(n=5000, base_rate=0.05, logit_scale=1.0, miscalibration=1.0):\n",
        "    # Generate true probabilities via latent logits\n",
        "    z = logit_scale * rng.standard_normal(n)\n",
        "    # shift to get desired base rate approximately\n",
        "    z = z + np.log(base_rate/(1-base_rate))\n",
        "    p_true = 1/(1+np.exp(-z))\n",
        "    y = (rng.random(n) < p_true).astype(int)\n",
        "    # observed model probs are miscalibrated by scaling logits\n",
        "    z_model = miscalibration * z\n",
        "    p_model = 1/(1+np.exp(-z_model))\n",
        "    return y, p_model, p_true\n",
        "\n",
        "y, p_model, p_true = make_probs(miscalibration=2.0)\n",
        "print('base_rate', y.mean())\n",
        "check('in_0_1', np.all((p_model>=0) & (p_model<=1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kEhnLfGgSIf"
      },
      "source": [
        "## Section 1 — Metrics\n",
        "\n",
        "### Task 1.1: Confusion matrix metrics at a threshold\n",
        "Implement precision, recall, F1 at threshold t.\n",
        "\n",
        "# HINT:\n",
        "- y_hat = (p>=t)\n",
        "- TP/FP/FN\n",
        "\n",
        "**Checkpoint:** Why is accuracy misleading under imbalance?\n",
        "\n",
        "**Answer:** Accuracy counts all correct predictions equally. Under class imbalance (for instance, 99% negatives, 1% positives), a trivial model that predicts everything as negative achieves 99% accuracy, yet it completely fails at the task of finding positives (TP = 0, recall = 0). Accuracy is dominated by the majority class (TN), so it hides poor performance on the minority class that usually matters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVXEWi3tgSIf",
        "outputId": "033f2aa2-c40e-435e-fe95-cb0822227f29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'tp': 2, 'fp': 2, 'fn': 354, 'recall': 0.005617977528089872, 'precision': 0.499999999999875, 'F1 score': 0.011111111111089075}\n"
          ]
        }
      ],
      "source": [
        "def metrics_at_threshold(y, p, t):\n",
        "    # TODO\n",
        "    y = y.astype(int)\n",
        "    y_hat = (p >= t).astype(int)\n",
        "    tp = int(np.sum((y_hat == 1) & (y == 1)))\n",
        "    fp = int(np.sum((y_hat == 1) & (y == 0)))\n",
        "    fn = int(np.sum((y_hat == 0) & (y == 1)))\n",
        "    # tn = int(np.sum((y_hat == 0) && (y == 0)))\n",
        "\n",
        "    precision = tp / (tp + fp + 1e-12)\n",
        "    recall = tp / (tp + fn + 1e-12)\n",
        "    F1_score = 2 * precision * recall / (precision + recall + 1e-12)\n",
        "    return {'tp':tp, 'fp':fp, 'fn':fn, 'recall':recall, 'precision':precision, 'F1 score':F1_score }\n",
        "\n",
        "m = metrics_at_threshold(y, p_model, t=0.5)\n",
        "print(m)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JwpVqzBgSIf"
      },
      "source": [
        "### Task 1.2: PR curve area (approx)\n",
        "Compute a simple PR-AUC approximation by sorting thresholds.\n",
        "\n",
        "# HINT:\n",
        "- sort by p desc\n",
        "- compute precision/recall at each cut\n",
        "\n",
        "**Interview Angle:** when is PR-AUC preferable to ROC-AUC?\n",
        "\n",
        "**Answer:** PR-AUC is preferable to ROC-AUC when the positive class is rare and we care about performance on positives (finding true cases and avoiding false alarms).\n",
        "- Severe class imbalance\n",
        "- False positives are costly\n",
        "- We care about “positive usefulness”\n",
        "- PR-AUC reflects real trade-offs between precision and recall WHERE AS ROC-AUC can look good even when no usable threshold exists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJN_1YAWgSIf",
        "outputId": "f2a6d181-3b72-4b4f-df7c-5794877f469d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pr_auc 0.20170158178004188\n",
            "OK: finite\n"
          ]
        }
      ],
      "source": [
        "def pr_curve(y, p):\n",
        "    # TODO: return arrays (recall, precision)\n",
        "    order = np.argsort(-p)\n",
        "    y_sorted = y[order]\n",
        "    tp = np.cumsum(y_sorted == 1)\n",
        "    fp = np.cumsum(y_sorted == 0)\n",
        "    # print(tp, fp)\n",
        "    precision = tp / (tp + fp + 1e-12)\n",
        "    recall = tp / (tp[-1] + 1e-12)\n",
        "    return recall, precision\n",
        "\n",
        "def auc_trapz(x, y):\n",
        "    # TODO\n",
        "    return float(np.trapezoid(y, x))\n",
        "\n",
        "rec, prec = pr_curve(y, p_model)\n",
        "pr_auc = auc_trapz(rec, prec)\n",
        "print('pr_auc', pr_auc)\n",
        "check('finite', np.isfinite(pr_auc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhuc4aYMgSIf"
      },
      "source": [
        "## Section 2 — Calibration\n",
        "\n",
        "### Task 2.1: Reliability curve + ECE\n",
        "\n",
        "Bin probabilities and compute:\n",
        "- avg predicted prob per bin\n",
        "- empirical accuracy per bin\n",
        "- ECE = sum (bin_weight * |acc - conf|)\n",
        "\n",
        "# HINT:\n",
        "- np.digitize\n",
        "\n",
        "**FAANG gotcha:** model can have good ranking but bad calibration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nl7yyAFLgSIg",
        "outputId": "f823a92b-81a0-45ec-c885-c27bfdf72935"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ECE 0.056267450617459955\n",
            "OK: ece_finite\n"
          ]
        }
      ],
      "source": [
        "def reliability_bins(y, p, n_bins=10):\n",
        "    # TODO: return (bin_acc, bin_conf, bin_frac)\n",
        "    y = y.astype(int)\n",
        "    edges = np.linspace(0, 1, n_bins + 1)\n",
        "    b = np.digitize(p, edges[1:-1], right=False)\n",
        "\n",
        "    bin_accuracy = np.zeros(n_bins)\n",
        "    bin_configuration = np.zeros(n_bins)\n",
        "    bin_fraction = np.zeros(n_bins)\n",
        "\n",
        "    for i in range(n_bins):\n",
        "      mask = (b == i)\n",
        "      if mask.any():\n",
        "        bin_accuracy[i] = y[mask].mean()\n",
        "        bin_configuration[i] = p[mask].mean()\n",
        "        bin_fraction[i] = mask.mean()\n",
        "\n",
        "    return bin_accuracy, bin_configuration, bin_fraction\n",
        "\n",
        "\n",
        "def ece(bin_acc, bin_conf, bin_frac):\n",
        "    # TODO\n",
        "    return float(np.sum(bin_frac * np.abs(bin_acc - bin_conf)))\n",
        "\n",
        "bin_acc, bin_conf, bin_frac = reliability_bins(y, p_model, n_bins=10)\n",
        "ECE = ece(bin_acc, bin_conf, bin_frac)\n",
        "print('ECE', ECE)\n",
        "check('ece_finite', np.isfinite(ECE))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9sJLbOlgSIg"
      },
      "source": [
        "### Task 2.2: Temperature scaling\n",
        "\n",
        "We assume p_model came from logits z_model. Approximate logits via logit(p).\n",
        "Then find temperature T that minimizes NLL on validation split: sigmoid(z/T).\n",
        "\n",
        "# HINT:\n",
        "- logit(p)=log(p/(1-p))\n",
        "- grid search T over [0.5..5]\n",
        "\n",
        "**Checkpoint:** Why does scaling logits preserve ranking?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuOkwnFkgSIg",
        "outputId": "d64e67bf-4a9b-4053-8002-d94507128bdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best_T 2.0612244897959187\n",
            "ECE_before 0.051093775041383536 ECE_after 0.014975308183525216\n"
          ]
        }
      ],
      "source": [
        "def logit(p, eps=1e-12):\n",
        "    p = np.clip(p, eps, 1-eps)\n",
        "    return np.log(p/(1-p))\n",
        "\n",
        "def nll(y, p, eps=1e-12):\n",
        "    p = np.clip(p, eps, 1-eps)\n",
        "    return float(-np.mean(y*np.log(p) + (1-y)*np.log(1-p)))\n",
        "\n",
        "# TODO: split into val and fit T\n",
        "idx = rng.permutation(len(y))\n",
        "val = idx[: len(y)//2]\n",
        "test = idx[len(y)//2:]\n",
        "\n",
        "z = logit(p_model)\n",
        "\n",
        "Ts = np.linspace(0.5, 5.0, 50)\n",
        "best_T = None\n",
        "best_loss = float('inf')\n",
        "for T in Ts:\n",
        "  pT = 1 / (1 + np.exp(-(z[val] / T)))\n",
        "  L = nll(y[val], pT)\n",
        "  if L < best_loss:\n",
        "    best_loss = L\n",
        "    best_T = T\n",
        "# p_cal = 1 / (1 + np.exp(-z[test] / best_T))\n",
        "\n",
        "print('best_T', best_T)\n",
        "# apply temperature on test\n",
        "p_cal = 1 / (1 + np.exp(-(z[test] / best_T)))\n",
        "ECE_before = ece(*reliability_bins(y[test], p_model[test], 10))\n",
        "ECE_after = ece(*reliability_bins(y[test], p_cal, 10))\n",
        "print('ECE_before', ECE_before, 'ECE_after', ECE_after)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKUfu0i4gSIg"
      },
      "source": [
        "## Section 3 — Thresholding with costs\n",
        "\n",
        "### Task 3.1: Pick threshold minimizing cost\n",
        "Cost = c_fp*FP + c_fn*FN\n",
        "\n",
        "# TODO: sweep thresholds and pick best.\n",
        "\n",
        "**Interview Angle:** map model probabilities to business decisions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf1_fYSLgSIg",
        "outputId": "6290e2c5-c194-44b0-fad9-278e84430db3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t* 0.006 cost 2626.0\n"
          ]
        }
      ],
      "source": [
        "def best_threshold_cost(y, p, c_fp=1.0, c_fn=10.0):\n",
        "    # TODO\n",
        "    ts = np.linspace(0, 1, 501)\n",
        "    best_T = 0.5\n",
        "    best_cost = float('inf')\n",
        "\n",
        "    for t in ts:\n",
        "      y_hat = (p >= t).astype(int)\n",
        "      fp = np.sum((y_hat == 1) & (y == 0))\n",
        "      fn = np.sum((y_hat == 0) & (y == 1))\n",
        "      cost = c_fp * fp + c_fn * fn\n",
        "\n",
        "      if cost < best_cost:\n",
        "        best_cost = cost\n",
        "        best_T = float(t)\n",
        "\n",
        "    return best_T, float(best_cost)\n",
        "\n",
        "t_star, cost_star = best_threshold_cost(y, p_model, c_fp=1.0, c_fn=10.0)\n",
        "print('t*', t_star, 'cost', cost_star)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBbfiwSMgSIg"
      },
      "source": [
        "---\n",
        "## Submission Checklist\n",
        "- All TODOs completed\n",
        "- ECE computed\n",
        "- Temperature scaling applied\n",
        "- Threshold recommendation written\n"
      ]
    }
  ]
}