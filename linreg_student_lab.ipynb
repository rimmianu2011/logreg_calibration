{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {"provenance": []},
    "kernelspec": {"name": "python3", "display_name": "Python 3"},
    "language_info": {"name": "python"}
  },
  "cells": [
    {"cell_type":"markdown","metadata":{},"source":[
      "# Linear Regression & GLMs — Student Lab\n",
      "\n",
      "Complete all TODOs. Avoid sklearn for core parts." 
    ]},
    {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
      "import numpy as np\n",
      "\n",
      "def check(name: str, cond: bool):\n",
      "    if not cond:\n",
      "        raise AssertionError(f'Failed: {name}')\n",
      "    print(f'OK: {name}')\n",
      "\n",
      "rng = np.random.default_rng(0)"
    ]},

    {"cell_type":"markdown","metadata":{},"source":[
      "## Section 0 — Synthetic Dataset (with collinearity)\n",
      "We generate data where features can be highly correlated to motivate ridge." 
    ]},
    {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
      "def make_regression(n=400, d=5, noise=0.5, collinear=True):\n",
      "    X = rng.standard_normal((n, d))\n",
      "    if collinear and d >= 2:\n",
      "        X[:, 1] = X[:, 0] * 0.95 + 0.05 * rng.standard_normal(n)\n",
      "    w_true = rng.standard_normal(d)\n",
      "    y = X @ w_true + noise * rng.standard_normal(n)\n",
      "    return X, y, w_true\n",
      "\n",
      "X, y, w_true = make_regression()\n",
      "n, d = X.shape\n",
      "check('shapes', y.shape == (n,))\n",
      "print('corr(x0,x1)=', np.corrcoef(X[:,0], X[:,1])[0,1])"
    ]},

    {"cell_type":"markdown","metadata":{},"source":[
      "## Section 1 — OLS Closed Form\n",
      "\n",
      "### Task 1.1: Closed-form w_hat using solve\n",
      "\n",
      "# TODO: compute w_hat using solve on (X^T X) w = X^T y\n",
      "# HINT: `XtX = X.T@X`, `Xty = X.T@y`, `np.linalg.solve(XtX, Xty)`\n",
      "\n",
      "**Checkpoint:** Why is explicit inverse discouraged?" 
    ]},
    {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
      "# TODO\n",
      "XtX = ...\n",
      "Xty = ...\n",
      "w_hat = ...\n",
      "\n",
      "check('w_shape', w_hat.shape == (d,))"
    ]},

    {"cell_type":"markdown","metadata":{},"source":[
      "### Task 1.2: Evaluate fit + residuals\n",
      "Compute:\n",
      "- predictions y_pred\n",
      "- MSE\n",
      "- residual mean and std\n",
      "\n",
      "**Interview Angle:** What does a structured residual pattern imply (e.g., nonlinearity)?" 
    ]},
    {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
      "# TODO\n",
      "y_pred = ...\n",
      "mse = ...\n",
      "resid = ...\n",
      "print('mse', mse, 'resid_mean', resid.mean(), 'resid_std', resid.std())\n",
      "check('finite', np.isfinite(mse))"
    ]},

    {"cell_type":"markdown","metadata":{},"source":[
      "## Section 2 — Gradient Descent\n",
      "\n",
      "### Task 2.1: Implement MSE loss + gradient\n",
      "\n",
      "Loss = mean((Xw-y)^2), grad = (2/n) X^T(Xw-y)\n",
      "\n",
      "# TODO: implement `mse_loss_and_grad`\n",
      "\n",
      "**FAANG gotcha:** shapes and constants." 
    ]},
    {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
      "def mse_loss_and_grad(X, y, w):\n",
      "    # TODO\n",
      "    ...\n",
      "\n",
      "w0 = np.zeros(d)\n",
      "loss0, g0 = mse_loss_and_grad(X, y, w0)\n",
      "check('grad_shape', g0.shape == (d,))\n",
      "check('finite_loss', np.isfinite(loss0))"
    ]},

    {"cell_type":"markdown","metadata":{},"source":[
      "### Task 2.2: Train with GD + compare to closed-form\n",
      "\n",
      "# TODO: implement a simple GD loop, track loss, and compare final weights to w_hat.\n",
      "\n",
      "**Checkpoint:** How does feature scaling affect GD?" 
    ]},
    {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
      "def train_gd(X, y, lr=0.05, steps=500):\n",
      "    # TODO\n",
      "    ...\n",
      "\n",
      "w_gd, losses = train_gd(X, y, lr=0.05, steps=500)\n",
      "print('final_loss', losses[-1])\n",
      "print('||w_gd-w_hat||', np.linalg.norm(w_gd - w_hat))\n",
      "check('loss_decreases', losses[-1] <= losses[0])"
    ]},

    {"cell_type":"markdown","metadata":{},"source":[
      "## Section 3 — Ridge Regression (L2)\n",
      "\n",
      "### Task 3.1: Ridge closed-form\n",
      "w = (X^T X + λI)^{-1} X^T y\n",
      "\n",
      "# TODO: implement ridge_solve\n",
      "\n",
      "**Interview Angle:** Why does ridge help under collinearity?" 
    ]},
    {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
      "def ridge_solve(X, y, lam):\n",
      "    # TODO\n",
      "    ...\n",
      "\n",
      "w_ridge = ridge_solve(X, y, lam=1.0)\n",
      "check('ridge_shape', w_ridge.shape == (d,))"
    ]},

    {"cell_type":"markdown","metadata":{},"source":[
      "### Task 3.2: Bias/variance demo with train/test split\n",
      "\n",
      "# TODO: split into train/test and compare MSE for multiple lambdas.\n",
      "\n",
      "**Checkpoint:** why can test error improve even when train error worsens?" 
    ]},
    {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
      "# TODO\n",
      "idx = rng.permutation(n)\n",
      "train = idx[: int(0.7*n)]\n",
      "test = idx[int(0.7*n):]\n",
      "Xtr, ytr = X[train], y[train]\n",
      "Xte, yte = X[test], y[test]\n",
      "\n",
      "lams = [0.0, 0.1, 1.0, 10.0]\n",
      "results = []\n",
      "for lam in lams:\n",
      "    w = ridge_solve(Xtr, ytr, lam=lam) if lam > 0 else np.linalg.solve(Xtr.T@Xtr, Xtr.T@ytr)\n",
      "    tr_mse = np.mean((Xtr@w - ytr)**2)\n",
      "    te_mse = np.mean((Xte@w - yte)**2)\n",
      "    results.append((lam, tr_mse, te_mse))\n",
      "print('lam, train_mse, test_mse')\n",
      "for r in results:\n",
      "    print(r)"
    ]},

    {"cell_type":"markdown","metadata":{},"source":[
      "## Section 4 — GLM Intuition\n",
      "\n",
      "### Task 4.1: Match tasks to (distribution, link)\n",
      "Fill in a table for:\n",
      "- regression\n",
      "- binary classification\n",
      "- count prediction\n",
      "\n",
      "**Explain:** what changes when you go from OLS to a GLM?" 
    ]},
    {"cell_type":"markdown","metadata":{},"source":[
      "| Problem | Target type | Distribution | Link | Loss |\n",
      "|---|---|---|---|---|\n",
      "| House price | continuous | ? | ? | ? |\n",
      "| Fraud | binary | ? | ? | ? |\n",
      "| Clicks per user | count | ? | ? | ? |\n"
    ]},

    {"cell_type":"markdown","metadata":{},"source":[
      "---\n",
      "## Submission Checklist\n",
      "- All TODOs completed\n",
      "- Train/test results shown for ridge\n",
      "- Short answers to checkpoint questions\n"
    ]}
  ]
}
